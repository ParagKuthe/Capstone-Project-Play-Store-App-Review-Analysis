{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "MSa1f5Uengrz",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "yiiVWRdJDDil",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO"
      ],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParagKuthe/Capstone-Project-Play-Store-App-Review-Analysis/blob/main/Email_Campaign_Effectiveness_Prediction_ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<font size='8px'><font color=#800080>**Project Name**</font> - <font color='#3792cb'>**Email Campaign Effectiveness Prediction**</font>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to create a machine learning model that can characterize and track emails sent through Gmail-based email marketing campaigns. This model will be used by small to medium business owners who are looking to improve the effectiveness of their email marketing efforts and increase customer retention.\n",
        "\n",
        "One of the main challenges in email marketing is determining which emails are being read, ignored, or acknowledged by the reader. By understanding which emails are most effective at engaging the reader, business owners can tailor their marketing efforts and increase their chances of success.\n",
        "\n",
        "To address this problem, we will gather data on a variety of email characteristics, including the subject line, sender name, email content, email format, and email frequency. We will also consider the target audience of the emails and any other relevant factors.\n",
        "\n",
        "Using this data, we will train a machine learning model to predict whether an email is likely to be read, ignored, or acknowledged by the reader. This model will be able to analyze new emails and provide a prediction of how they are likely to be received by the reader.\n",
        "\n",
        "To evaluate the performance of the model, we will split our data into a training set and a testing set. We will use the training set to fit the model and the testing set to evaluate its performance. We will use a variety of metrics, such as precision, recall, and F1 score, to assess the model's accuracy and effectiveness.\n",
        "\n",
        "Once the model is trained and evaluated, it can be deployed in a production environment to help small to medium business owners improve the effectiveness of their email marketing campaigns. By using the model to characterize and track emails, they will be able to make more informed decisions about how to target their marketing efforts and increase customer retention.\n",
        "\n",
        "Overall, this project aims to provide small to medium business owners with a powerful tool for improving the effectiveness of their email marketing campaigns. By using machine learning to characterize and track emails, they will be able to make more informed decisions and increase the chances of success for their marketing efforts."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small to medium business owners are using Gmail-based email marketing strategies to convert prospective customers into leads, but they are unable to track which emails are being ignored, read, or acknowledged by the reader. They want to create a machine learning model to help characterize and track these emails. The main objective is to improve the effectiveness of their email marketing efforts and increase customer retention."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "from numpy import loadtxt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import *\n",
        "from scipy import stats\n",
        "import math\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score,confusion_matrix, f1_score, classification_report, roc_auc_score\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/data_email_campaign.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(\"Information Of Dataset:\\n\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Count Of Duplicate Values:\\n\")\n",
        "duplicate=df.duplicated()\n",
        "df[duplicate].sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"Null Values In DataSet\\n\")\n",
        "null_values=pd.DataFrame(df.isnull().sum().reset_index(name=\"Null Values\"))\n",
        "null_values=null_values[null_values[\"Null Values\"]!=0]\n",
        "null_values_percentage=null_values[\"Null Values\"]/len(df)*100\n",
        "pd.DataFrame(null_values)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing Missing Values\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "vNDtK-B2s2W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "bYD-pTFdgPu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the small to medium business owners are making effective use of Gmail-based Email marketing Strategies for offline targeting of converting their prospective customers into leads so that they stay with them in Business. The main objective is to create a machine learning model to characterize the mail and track the mail that is ignored; read; acknowledged by the reader.\n",
        "\n",
        "In the above dataset there are total 68353 rows and  12 columns with null values and 48291 rows and 12 columns after removing null values"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Columns In DataSets:\\n\")\n",
        "list(df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include=\"all\")"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Email_Id :** Email Id Of Customer \\\n",
        "**Email_Type :** 1 For marketing Emails & 2 for business Emails \\\n",
        "**Subject_Hotness_Score :** Score on the basis of how good and effective the content is. \\\n",
        "**Email_Source_Type :** Source of the Emails like 1 for marketing or 2 for business. \\\n",
        "**Customer_Location :** Location of the customer. \\\n",
        "**Email_Campaign_Type :** Type of Email 1 for marketing,2 for business, 3 for important admin mails related to products. \\\n",
        "**Total_Past_Communications :** The total previous mails from the same source the no of communication had. \\\n",
        "**Time_Email_sent_Category :** The time at which the mail was sent. 1 for morning, 2 for evening, 3 for night. \\\n",
        "**Word_Count :** The no of words contain the mail. \\\n",
        "**Total_Links :** Number Of links in the mails. \\\n",
        "**Total_Images :** Number of images in the link. \\\n",
        "**Email_Status :** target variable which contain 0 for the mail was ignored, 1 for read, 2 for acknowledged by reader."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df:\n",
        "  print(\"\\n Unique Values In \",i,\"\\n\",df[i].unique(),\"\\nWith Count Of:\",df[i].nunique())"
      ],
      "metadata": {
        "id": "rJcNmCqPqW2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Column Having Categorical values\n",
        "df1=df.drop([\"Email_ID\",\"Customer_Location\"],axis=1)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ignored=df1[df1[\"Email_Status\"]==0]\n",
        "read=df1[df1[\"Email_Status\"]==1]\n",
        "acknowledge=df1[df1[\"Email_Status\"]==2]\n",
        "print(\"Emails Ignored By The User :\",len(ignored))\n",
        "print(\"Emails Read By The User :\",len(read))\n",
        "print(\"Emails Acknowledge By The User :\",len(acknowledge))\n"
      ],
      "metadata": {
        "id": "qGWANNE1xE1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ignored Emails Details\n",
        "ignored.describe()"
      ],
      "metadata": {
        "id": "IDLXExrey_MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read Emails Details\n",
        "read.describe()"
      ],
      "metadata": {
        "id": "zmBehMPo0A6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#acknowledge Emails Details\n",
        "acknowledge.describe()"
      ],
      "metadata": {
        "id": "RXg2vGFF0Kmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "p = df.groupby(\"Time_Email_sent_Category\").size().reset_index(name='Count')\n",
        "\n",
        "# Display the result\n",
        "print(p)\n"
      ],
      "metadata": {
        "id": "Vb1XV_NaE7dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Email_source_type_count_on_Email_type=pd.DataFrame(df.groupby(\"Email_Type\")[\"Email_Source_Type\"].value_counts().reset_index(name=\"Count\"))\n",
        "Email_source_type_count_on_Email_type"
      ],
      "metadata": {
        "id": "txGKT8qqphCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Email_Campaign_Type_count_on_Email_type=pd.DataFrame(df.groupby(\"Email_Type\")[\"Email_Campaign_Type\"].value_counts().reset_index(name=\"Count\"))\n",
        "Email_Campaign_Type_count_on_Email_type"
      ],
      "metadata": {
        "id": "JiBDO4vqtEOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Customer_Location_on_Email_type=pd.DataFrame(df.groupby(\"Email_Type\")[\"Customer_Location\"].value_counts().reset_index(name=\"Count\"))\n",
        "Customer_Location_on_Email_type"
      ],
      "metadata": {
        "id": "0o_SpIxpviqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Time_Email_sent_Category_on_Email_type=pd.DataFrame(df.groupby(\"Email_Type\")[\"Time_Email_sent_Category\"].value_counts().reset_index(name=\"Count\"))\n",
        "Time_Email_sent_Category_on_Email_type"
      ],
      "metadata": {
        "id": "UKfp9Bwrzbsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features=df.select_dtypes(include=[\"float\",\"int\"]).columns.to_list()\n",
        "num_features"
      ],
      "metadata": {
        "id": "O1xZrlMG84XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('Email_Type')[num_features].agg([\"sum\",\"mean\",\"median\",\"count\"]).T"
      ],
      "metadata": {
        "id": "gTkpUh7V-pm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\"Subject_Hotness_Score\")[num_features].agg([\"sum\",\"mean\",\"median\",\"count\"]).T"
      ],
      "metadata": {
        "id": "c0q3ZhU6_fnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\"Email_Source_Type\")[num_features].agg([\"sum\",\"mean\",\"median\",\"count\"]).T"
      ],
      "metadata": {
        "id": "wmvdqf6t_3ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\"Email_Campaign_Type\")[num_features].agg([\"sum\",\"mean\",\"median\",\"count\"]).T"
      ],
      "metadata": {
        "id": "vq36SVyX_-of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\"Time_Email_sent_Category\")[num_features].agg([\"sum\",\"mean\",\"median\",\"count\"]).T"
      ],
      "metadata": {
        "id": "K2t4RG9yAE5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\"Email_Status\")[num_features].agg([\"sum\",\"mean\",\"median\",\"count\"]).T"
      ],
      "metadata": {
        "id": "3RzONGL8AfJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features=[feature for feature in df.columns.to_list() if feature not in num_features]\n",
        "cat_features"
      ],
      "metadata": {
        "id": "Rge5k4fQ-VI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and it contains a column named 'Email_Status'\n",
        "value_counts = df['Email_Status'].value_counts().reset_index()\n",
        "\n",
        "# Rename the columns in the new DataFrame\n",
        "value_counts.columns = ['Email_Status', 'Count']\n",
        "\n",
        "# Create the Email_status_value_count DataFrame\n",
        "Email_status_value_count = pd.DataFrame(value_counts)\n",
        "\n",
        "# Display the result\n",
        "print(Email_status_value_count)\n"
      ],
      "metadata": {
        "id": "1aE8hgzP1K5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Email_status_value_count_on_Email_Type=pd.DataFrame(df.groupby(\"Email_Type\")[\"Email_Status\"].value_counts().reset_index(name=\"count\"))\n",
        "Email_status_value_count_on_Email_Type"
      ],
      "metadata": {
        "id": "3-4RQaODn-uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Total_link_on_Email_Type=pd.DataFrame(df.groupby(\"Email_Type\")[\"Total_Links\"].value_counts().reset_index(name=\"count\"))\n",
        "Total_link_on_Email_Type"
      ],
      "metadata": {
        "id": "CIe6K3zb6TC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Subject_Hotness_Score_on_Email_Source_Type=pd.DataFrame(df.groupby(\"Email_Source_Type\")[\"Subject_Hotness_Score\"].value_counts().reset_index(name=\"Count\"))\n",
        "Subject_Hotness_Score_on_Email_Source_Type"
      ],
      "metadata": {
        "id": "CI3OwMr7DhA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Customer_Location_on_Email_Source_Type=pd.DataFrame(df.groupby(\"Email_Source_Type\")[\"Customer_Location\"].value_counts().reset_index(name=\"Count\"))\n",
        "Customer_Location_on_Email_Source_Type"
      ],
      "metadata": {
        "id": "z7tIbksHEEFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Email_Campaign_Type_on_Email_Source_Type=pd.DataFrame(df.groupby(\"Email_Source_Type\")[\"Email_Campaign_Type\"].value_counts().reset_index(name=\"Count\"))\n",
        "Email_Campaign_Type_on_Email_Source_Type"
      ],
      "metadata": {
        "id": "qQkqAvS2EQ58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Time_Email_sent_Category_on_Email_Source_Type=pd.DataFrame(df.groupby(\"Email_Source_Type\")[\"Time_Email_sent_Category\"].value_counts().reset_index(name=\"Count\"))\n",
        "Time_Email_sent_Category_on_Email_Source_Type"
      ],
      "metadata": {
        "id": "DFOiRz-WEemb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Email_Status_on_Email_Source_Type=pd.DataFrame(df.groupby(\"Email_Source_Type\")[\"Email_Status\"].value_counts().reset_index(name=\"Count\"))\n",
        "Email_Status_on_Email_Source_Type"
      ],
      "metadata": {
        "id": "CHgKp4fvEpD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Avg_Link=df.groupby(\"Email_Type\")[\"Total_Links\"].mean().reset_index(name=\"Avg Link\")\n",
        "Avg_Word_Count=df.groupby(\"Email_Type\")[\"Word_Count\"].mean().reset_index(name=\"Avg_Word_Count\")\n",
        "Avg_Images=df.groupby(\"Email_Type\")[\"Total_Images\"].mean().reset_index(name=\"Avg Images\")"
      ],
      "metadata": {
        "id": "Idi7X-xVPeMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "data=df[\"Email_Type\"].value_counts()\n",
        "\n",
        "labels=[\"Marketting Email\",\"Business Email\"]\n",
        "plt.pie(data,labels=labels,autopct=\"%.1f%%\",colors=[\"cyan\", \"Green\"])\n",
        "plt.title('Type Of The Emails ',size=15,loc='center')\n",
        "plt.legend(bbox_to_anchor=(0.9, 0, 0.78, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For analysing the types of the emails out of 100% pie chart is best suitable for such type of visualisation"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Of the types of the Emails are the marketting Email having total percentage of 71% and business Email having percentage out of total is 29%."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights helps in creating a positive business impact. From thise pie chart we know that the most of the emails are marketting email which means the most of the business owener positively uses emails for business purpose and marketting thier product on email"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "data=df[\"Email_Source_Type\"].value_counts()\n",
        "\n",
        "labels=[\"Marketting Email\",\"Business Emails\"]\n",
        "plt.pie(data,labels=labels,autopct=\"%.1f%%\")\n",
        "plt.title('Email Source Type',size=15,loc='center')\n",
        "plt.legend(bbox_to_anchor=(0.9, 0, 0.78, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For analysing the emails source type out of 100% pie chart is best suitable for such type of visualisation"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Of the Email source types are the marketting Email having total percentage of 54% and business emails having percentage out of total is 46%."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights helps in creating a positive business impact. From thise pie chart we know that the most of the emails are marketting email which means the most of the business owener positively uses emails for business purpose and marketting thier product on email"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize = (7,7))\n",
        "plt.title(\"Count Of Email_Source_Type On The Basis Of Email_Type\")\n",
        "ax = sns.barplot(data = Email_source_type_count_on_Email_type, x = 'Email_Source_Type',y = \"Count\", hue = \"Email_Type\")\n",
        "plt.show()\n",
        "print(\"---------------------------------------------------------------------------------\")\n",
        "\n",
        "#  visualization code\n",
        "plt.figure(figsize = (7,7))\n",
        "plt.title(\"Count Of Email_Campaign_Type On The Basis Of Email_Type\")\n",
        "ax = sns.barplot(data = Email_Campaign_Type_count_on_Email_type, x = 'Email_Campaign_Type',y = \"Count\", hue = \"Email_Type\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"---------------------------------------------------------------------------------\")\n",
        "\n",
        "plt.figure(figsize = (7,7))\n",
        "plt.title(\"Count Of Customer_Location On The Basis Of Email_Type\")\n",
        "ax = sns.barplot(data = Customer_Location_on_Email_type, x = 'Customer_Location',y = \"Count\", hue = \"Email_Type\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"---------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "# Get unique values in the 'Email_Type' column\n",
        "unique_email_types = Time_Email_sent_Category_on_Email_type['Email_Type'].unique()\n",
        "\n",
        "# Define a custom color palette for all unique values in 'Email_Type'\n",
        "custom_palette = {email_type: sns.color_palette(\"husl\")[i] for i, email_type in enumerate(unique_email_types)}\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.title(\"Count Of Time_Email_sent_Category On The Basis Of Email_Type\")\n",
        "ax = sns.barplot(data=Time_Email_sent_Category_on_Email_type, x='Time_Email_sent_Category', y='Count', hue='Email_Type', palette=custom_palette)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"---------------------------------------------------------------------------------\")\n",
        "\n",
        "plt.figure(figsize = (7,7))\n",
        "plt.title(\"Count Of Total link On The Basis Of Email_Type\")\n",
        "ax = sns.barplot(data = Total_link_on_Email_Type, x = \"Total_Links\",y = \"count\", hue = 'Email_Type')\n",
        "plt.xticks(rotation=50, ha='right')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For comapring the different parameters of the dataset bar chart is used which better visualise the result and easy for understanding."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In above visualisation we are plotting diiferent variables of the dataset with respect to the Email_types. and understanding hows the correlation between Email_types and other variables"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights helps in creating a positive business impact.In above visualisation we are plotting diiferent variables of the dataset with respect to the Email_types. and understanding hows the correlation between Email_types and other variables"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "p = df.groupby(\"Customer_Location\").agg({\"Total_Past_Communications\": sum}).reset_index()\n",
        "\n",
        "# Convert to DataFrame\n",
        "p_df = pd.DataFrame(p)\n",
        "\n",
        "# Plotting a bar plot\n",
        "plt.bar(p_df['Customer_Location'], p_df['Total_Past_Communications'],color=\"green\")\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Customer Location')\n",
        "plt.ylabel('Total Past Communications')\n",
        "plt.title('Total Past Communications by Customer Location')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=df[\"Customer_Location\"].value_counts()\n",
        "\n",
        "labels=['G','E', 'D','C','F','B','A']\n",
        "plt.pie(data,labels=labels,autopct=\"%.1f%%\",explode=[0.01,0.01,0.01,0.01,0.01,0.3,0.01],colors=['skyblue','red','green','violet','pink','brown','cyan','magenta'])\n",
        "plt.title('Customer Location',size=15,loc='center')\n",
        "plt.legend(bbox_to_anchor=(0.9, 0, 0.78, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ip2NO_Q6CPix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ For comapring the different parameters of the dataset bar chart is used which better visualise the result and easy for understanding. \\\n",
        "+ For analysing the emails source type out of 100% pie chart is best suitable for such type of visualisation"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart 1: From thise chart we can know that the highest no of the total past communication is done with the G group customer location. \\\n",
        "chart 2 : From thise chart we know that the G community has maximum population out of all total  which is 40.9 % and A community has lowest population which is 2.5 %\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights helps in creating a positive business impact. the G community has maximum population out of all total which is 40.9 % and A community has lowest population which is 2.5%. From thise we can give more focused on customers with highest population community which ultimately provides positive business growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "p = df.groupby(\"Time_Email_sent_Category\").size().reset_index(name='Count')\n",
        "p\n",
        "# Assuming p_df is your DataFrame\n",
        "plt.bar(p['Time_Email_sent_Category'], p['Count'], width=0.3,color=\"blue\")  # Adjust the width as needed\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Time Email Sent Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Count of Emails by Time Email Sent Category')\n",
        "\n",
        "# Replace x-axis labels with desired labels\n",
        "plt.xticks(p['Time_Email_sent_Category'], ['Morning',  'Evening', 'Night'])\n",
        "\n",
        "# Rotating x-axis labels for better readability (optional)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For comapring the different parameters of the dataset bar chart is used which better visualise the result and easy for understanding."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest no of the mails send to the customers during evening time."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights helps in creating a positive business impact."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "Email_Status_Pie_Chart=df[\"Email_Status\"].value_counts()\n",
        "labels=(\"Ignored\",\"Read\",\"Acknowledge\")\n",
        "plt.pie(Email_Status_Pie_Chart,labels=labels,autopct=\"%.1f%%\",shadow=True, explode=[0,0,0],colors=[\"Red\",\"Orange\",\"skyblue\"])\n",
        "plt.title(\"Pie Distribution Of Email Status\")\n",
        "plt.legend(bbox_to_anchor=(0.9, 0, 0.78, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For analysing the emails status type out of 100% pie chart is best suitable for such type of visualisation"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From thise pie chart we know that the percentage of the email status.\\\n",
        "The percentage of Ignored are highest which is 80%. \\\n",
        "The percentage of mails which are read by user are 16% . \\\n",
        "The percentage of mails which are acknowledge by user are 4 %.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights helps in creating a positive business impact. From thise chart we know that percentage of emails which are opened, read and ackowledge by the user"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "custom_palette = [\"Red\", \"Blue\", \"cyan\"]\n",
        "\n",
        "plt.title(\"Email status value count on The Basis Of Email Type\")\n",
        "\n",
        "# Make sure to use the correct column names in the plot\n",
        "sns.barplot(data=Email_status_value_count_on_Email_Type, x=\"Email_Type\", y=\"count\", hue=\"Email_Status\", palette=custom_palette)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For comapring the different parameters of the dataset bar chart is used which better visualise the result and easy for understanding."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are analysing the count of email status such as emails which are ignored,opened and acknowledge by the user with respect to the email type."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights helps in creating a positive business impact. As we are analysing the count of  email status such as emails which are ignored,opened and acknowledge by the user with respect to the email type."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have defined your DataFrame named Customer_Location_on_Email_Source_Type\n",
        "\n",
        "# Get unique values in the 'Email_Source_Type' column\n",
        "unique_email_source_types = Customer_Location_on_Email_Source_Type['Email_Source_Type'].unique()\n",
        "\n",
        "# Define a custom color palette for Email_Source_Type\n",
        "custom_palette = {email_source_type: sns.color_palette(\"husl\")[i] for i, email_source_type in enumerate(unique_email_source_types)}\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.title(\"Count Of Email_Source_Type On The Basis Of Customer_Location\")\n",
        "ax = sns.barplot(data=Customer_Location_on_Email_Source_Type, x='Customer_Location', y=\"Count\", hue='Email_Source_Type', palette=custom_palette)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.title(\"Count Of Email_Source_Type On The Basis Of Email_Campaign_Type\")\n",
        "ax = sns.barplot(data=Email_Campaign_Type_on_Email_Source_Type, x='Email_Campaign_Type', y=\"Count\", hue='Email_Source_Type', palette=custom_palette)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.title(\"Count Of Time Email sent Category On The Basis Of Email_Campaign_Type\")\n",
        "ax = sns.barplot(data=Time_Email_sent_Category_on_Email_Source_Type, x='Time_Email_sent_Category', y=\"Count\", hue='Email_Source_Type', palette=custom_palette)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.title(\"Count Of Time Email Status On The Basis Of Email_Campaign_Type\")\n",
        "ax = sns.barplot(data=Email_Status_on_Email_Source_Type, x='Email_Status', y=\"Count\", hue='Email_Source_Type', palette=custom_palette)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For comapring the different parameters of the dataset bar chart is used which better visualise the result and easy for understanding."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are trying to find the correlation between different variables with respect to the Email Source Type"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights helps creating a positive business impact. Beacuse We are trying to find the correlation between different variables with respect to the Email Source Type."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize = (7,7))\n",
        "plt.title(\"Avg Link VS Email Type\")\n",
        "ax = sns.barplot(data = Avg_Link, x = \"Email_Type\",y = \"Avg Link\")\n",
        "plt.show()\n",
        "\n",
        "print(\"------------------------------------------------------------------\")\n",
        "\n",
        "plt.figure(figsize = (7,7))\n",
        "plt.title(\"Avg Word_Count  VS Email Type\")\n",
        "ax = sns.barplot(data = Avg_Word_Count, x = \"Email_Type\",y = \"Avg_Word_Count\")\n",
        "plt.show()\n",
        "\n",
        "print(\"------------------------------------------------------------------\")\n",
        "\n",
        "plt.figure(figsize = (7,7))\n",
        "plt.title(\"Avg Images VS Email_Type\")\n",
        "ax = sns.barplot(data = Avg_Images, x = 'Email_Type',y = \"Avg Images\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For comapring the different parameters of the dataset bar chart is used which better visualise the result and easy for understanding."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are trying to find the correlation between different variables with respect to the Email Type"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights helps in creating a positive business impact. Becuase the correlation between different variables with respect to the Email Type helps in finding the relation and its effect on different variables"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "for features in num_features:\n",
        "  sns.barplot(data=df,x=\"Email_Type\",y=features,hue=\"Customer_Location\")\n",
        "  plt.show()\n",
        "  plt.title(features)\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(data=df.corr(),annot=True,cmap=\"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For finding the relation between different different variables heatmap is used which best represents the relation between different variables"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From thise heatmap we know that what is the actual relation between different variables of the datasets"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "columns = ['Email_Type', 'Subject_Hotness_Score', 'Email_Source_Type',\n",
        "       'Customer_Location', 'Email_Campaign_Type', 'Total_Past_Communications',\n",
        "       'Time_Email_sent_Category','Email_Status']\n",
        "data_for_pairplot = df[columns]\n",
        "\n",
        "p = sns.pairplot(data_for_pairplot)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zb6jYdiQXdaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart I got to know, there are less linear relationship between variables and since most of them were categorial data with one or two category, it does not show much relationship.\n",
        "\n",
        "Total links and total image show some linear relation and we already know they are correlated as seen in earlier heatmap."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Email_Type of the campaign will not have any significant impact on the Email_Status.\n",
        "* The Subject_Hotness_Score of the email will not have any significant impact on the Total_Past_Communications.\n",
        "* The Customer_Location will not have any significant impact on the Total_Links and Total_Images in the email."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null Hypothesis: There is no relationship between Email_Type and Email_Status (H0: B1 = 0)\n",
        "* Alternative Hypothesis: There is a relationship between Email_Type and Email_Status (H1: B1  0)\n",
        "* Test Type : chi-square test"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# perform chi-square test of independence\n",
        "chi2, p_value, dof, expected = stats.chi2_contingency(pd.crosstab(df['Email_Type'], df['Email_Status']))\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis - the Email_Type has a significant impact on the Email_Status\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis - the Email_Type does not have a significant impact on the Email_Status\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this hypothesis, I used chi-square test of independence which is a statistical test to determine if there is a significant association between two categorical variables. In this case, the two variables are Email_Type and Email_Status."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test is appropriate because the variables are categorical and I want to determine if there is a relationship between them."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null Hypothesis: There is no linear relationship between Subject_Hotness_Score and Total_Past_Communications (H0:  = 0)\n",
        "* Alternative Hypothesis: There is a linear relationship between Subject_Hotness_Score and Total_Past_Communications (H1:   0)\n",
        "* Test Type : Pearson's correlation test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to perform the test, variable I used has null or infinity values, therefore creating copy and treating those\n",
        "data = df.copy()\n",
        "data = data.replace([np.inf, -np.inf], np.nan)\n",
        "data = data.fillna(data.mean())"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# perform correlation test\n",
        "r, p_value = stats.pearsonr(data['Subject_Hotness_Score'], data['Total_Past_Communications'])\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis - the Subject_Hotness_Score has a significant impact on the Total_Past_Communications\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis - the Subject_Hotness_Score does not have a significant impact on the Total_Past_Communications\")\n"
      ],
      "metadata": {
        "id": "qobSvZC4HZWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this hypothesis, I used Pearson's correlation test which measures the linear correlation between two continuous variables. In this case, the two variables are Subject_Hotness_Score and Total_Past_Communications."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test is appropriate because the variables are continuous and I want to determine if there is a linear relationship between them."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null Hypothesis: The mean of Total_Links is equal among the location (A, B, C, D, E, F, G) (H0: 1 = 2 = 3 = 4 = 5 = 6 = 7)\n",
        "* Alternative Hypothesis: The mean of Total_Links is not equal among the location (A, B, C, D, E, F, G) (H1: at least one mean is different from the others)\n",
        "* Test Type : ANOVA Test"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# perform ANOVA test\n",
        "f_value, p_value = stats.f_oneway(df[df['Customer_Location'] == 'A']['Total_Links'],\n",
        "                                  df[df['Customer_Location'] == 'B']['Total_Links'],\n",
        "                                  df[df['Customer_Location'] == 'C']['Total_Links'],\n",
        "                                  df[df['Customer_Location'] == 'D']['Total_Links'],\n",
        "                                  df[df['Customer_Location'] == 'E']['Total_Links'],\n",
        "                                  df[df['Customer_Location'] == 'F']['Total_Links'],\n",
        "                                  df[df['Customer_Location'] == 'G']['Total_Links'])\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis - the Customer_Location has a significant impact on the Total_Links in the email\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis - the Customer_Location does not have a significant impact on the Total_Links in the email\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# perform ANOVA test\n",
        "f_value, p_value = stats.f_oneway(df[df['Customer_Location'] == 'A']['Total_Images'],\n",
        "                                  df[df['Customer_Location'] == 'B']['Total_Images'],\n",
        "                                  df[df['Customer_Location'] == 'C']['Total_Images'],\n",
        "                                  df[df['Customer_Location'] == 'D']['Total_Images'],\n",
        "                                  df[df['Customer_Location'] == 'E']['Total_Images'],\n",
        "                                  df[df['Customer_Location'] == 'F']['Total_Images'],\n",
        "                                  df[df['Customer_Location'] == 'G']['Total_Images'])\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis - the Customer_Location has a significant impact on the Total_Images in the email\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis - the Customer_Location does not have a significant impact on the Total_Images in the email\")\n"
      ],
      "metadata": {
        "id": "GdUUsXddH4Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform Kruskal-Wallis test\n",
        "stat, p_value = stats.kruskal(df[df['Customer_Location'] == 'A']['Total_Links'],\n",
        "                              df[df['Customer_Location'] == 'B']['Total_Links'],\n",
        "                              df[df['Customer_Location'] == 'C']['Total_Links'],\n",
        "                              df[df['Customer_Location'] == 'D']['Total_Links'],\n",
        "                              df[df['Customer_Location'] == 'E']['Total_Links'],\n",
        "                              df[df['Customer_Location'] == 'F']['Total_Links'],\n",
        "                              df[df['Customer_Location'] == 'G']['Total_Links'])\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis - the Customer_Location has a significant impact on the Total_Links in the email\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis - the Customer_Location does not have a significant impact on the Total_Links in the email\")\n"
      ],
      "metadata": {
        "id": "1qamvH4KH1Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this hypothesis, I used ANOVA (Analysis of Variance) test because ANOVA is a statistical test that is used to determine whether there is a statistically significant difference in the means of two or more groups."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test is used to determine if there are significant differences between the means of two or more groups. In this case, we have different locations (A,B,C,D,E,F,G) and we want to determine if there is a significant difference in the mean of Total_Links among these groups. ANOVA is appropriate for this case because the variable Total_Links is continuous and we want to compare the means of multiple groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.dropna().inplace=True\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here not using any missing value imputation techniques.Becuase any imputation technique can influence the data towards a specific variables which drops down the accuracy of the ml model and it performs poorly. we are deleting all the null values and making our dataset completely perfect for ml model creation."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df1[\"Email_Type\"],df1[\"Subject_Hotness_Score\"])\n",
        "plt.xlabel(\"Email Type\")\n",
        "plt.ylabel(\"Subject Hotness Score\")\n",
        "plt.title(\"Email Type VS Subject Hotness Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OoShwIl8jwGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df1[\"Email_Type\"],df1[\"Email_Source_Type\"])\n",
        "plt.xlabel(\"Email Type\")\n",
        "plt.ylabel(\"Email Source Type\")\n",
        "plt.title(\"Email Type VS Email Source Type\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DSlsvi1jxwXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df1[\"Email_Type\"],df1[\"Email_Campaign_Type\"])\n",
        "plt.xlabel(\"Email Type\")\n",
        "plt.ylabel(\"Email Campaign Type\")\n",
        "plt.title(\"Email Type VS Email Campaign Type\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u8Cn6EO1xzIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df1[\"Email_Type\"],df1[\"Total_Past_Communications\"])\n",
        "plt.xlabel(\"Email Type\")\n",
        "plt.ylabel(\"Total Past Communications\")\n",
        "plt.title(\"Email Type VS Total Past Communications\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JYQly30Zx1yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df1[\"Email_Type\"],df1[\"Time_Email_sent_Category\"])\n",
        "plt.xlabel(\"Email Type\")\n",
        "plt.ylabel(\"Time Email sent Category\")\n",
        "plt.title(\"Email Type VS Time Email sent Category\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b_CnChx-x4cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df1[\"Email_Type\"],df1[\"Word_Count\"])\n",
        "plt.xlabel(\"Email Type\")\n",
        "plt.ylabel(\"Word Count\")\n",
        "plt.title(\"Email Type VS Word Coun\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M2-8d_vBx60f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df1[\"Email_Type\"],df1[\"Total_Links\"])\n",
        "plt.xlabel(\"Email_Type\")\n",
        "plt.ylabel(\"Total_Links\")\n",
        "plt.title(\"Email_Type VS Total_Links\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V6JazerLx9RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df1[\"Email_Type\"],df1[\"Total_Images\"])\n",
        "plt.xlabel(\"Email Type\")\n",
        "plt.ylabel(\"Total_Images\")\n",
        "plt.title(\"Email Type VS Total_Images\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sDJllCM8yAeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df1[\"Email_Campaign_Type\"],df1[\"Email_Status\"])\n",
        "plt.xlabel(\"Email Campaign Type\")\n",
        "plt.ylabel(\"Email Status\")\n",
        "plt.title(\"Email Campaign Type VS Email Status \")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plU6KiwlyDQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments in Email_Type\n",
        "mean=df1[\"Email_Type\"].mean()\n",
        "std=df1[\"Email_Type\"].std()\n",
        "outlier=mean + 2 * std\n",
        "df1[df1[\"Email_Type\"]>outlier]"
      ],
      "metadata": {
        "id": "NRv1-FHo1qs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments in Email_Source_Type\n",
        "mean=df1[\"Email_Source_Type\"].mean()\n",
        "std=df1[\"Email_Source_Type\"].std()\n",
        "outlier=mean + 2 * std\n",
        "df1[df1[\"Email_Source_Type\"]>outlier]"
      ],
      "metadata": {
        "id": "r7zYhd1kw4yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments In Email_Campaign_Type\n",
        "mean=df1[\"Email_Campaign_Type\"].mean()\n",
        "std=df1[\"Email_Campaign_Type\"].std()\n",
        "outlier=mean + 2 * std\n",
        "df1[df1[\"Email_Campaign_Type\"]>outlier]"
      ],
      "metadata": {
        "id": "VXN0FpYuxDQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments In Time_Email_sent_Category\n",
        "mean=df1[\"Time_Email_sent_Category\"].mean()\n",
        "std=df1[\"Time_Email_sent_Category\"].std()\n",
        "outlier=mean + 2 * std\n",
        "df1[df1[\"Time_Email_sent_Category\"]>outlier]"
      ],
      "metadata": {
        "id": "WWMtkmcgxIt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There Are No Any Outliers In Email Campaign Effectiveness Predidiction Dataset"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "#Converting Total_Past_Communications datatype from float to int\n",
        "df[\"Total_Past_Communications\"]=df[\"Total_Past_Communications\"].astype(\"int\")\n",
        "\n",
        "#Converting Total_Links datatype from float to int\n",
        "df[\"Total_Links\"]=df[\"Total_Links\"].astype(\"int\")\n",
        "\n",
        "#Converting Total_Links datatype from float to int\n",
        "df[\"Total_Images\"]=df[\"Total_Images\"].astype(\"int\")"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convered \"Float\" Data Type Of Column Variable Name \"Total_Past_Communications\", \"Total_Links\", \"Total_Images\" To \"Int\" Data Type"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "#Total_Image_link\n",
        "df[\"Total_Image_link\"]=df[\"Total_Links\"]+df[\"Total_Images\"]\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Avg Word count per image and link\n",
        "df[\"Avg Word count per image and link\"]=(df[\"Total_Image_link\"]/df[\"Word_Count\"]) * 100"
      ],
      "metadata": {
        "id": "Vou_f4xWVmsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Per Link\n",
        "df[\"Word Per Link\"]=(df[\"Total_Links\"]/df[\"Word_Count\"])"
      ],
      "metadata": {
        "id": "6nyMzq2eVmQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Per Image\n",
        "df[\"Word Per Image\"]=(df[\"Total_Images\"]/df[\"Word_Count\"])"
      ],
      "metadata": {
        "id": "evviFfOAVmDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping Constant and Quasi Constant Feature\n",
        "def dropping_constant(data):\n",
        "  from  sklearn.feature_selection import VarianceThreshold\n",
        "  var_thres= VarianceThreshold(threshold=0.05)\n",
        "  var_thres.fit(data)\n",
        "  concol = [column for column in data.columns\n",
        "          if column not in data.columns[var_thres.get_support()]]\n",
        "          #var_thres.get_support() return boolean values on checking condition\n",
        "  if \"Email_Status\" in concol:\n",
        "    concol.remove(\"Email_Status\")\n",
        "  else:\n",
        "    pass\n",
        "  print(f'Columns dropped: {concol}')\n",
        "  df_removed_var=data.drop(concol,axis=1)\n",
        "  return df_removed_var"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=df1.drop(\"Email_Status\",axis=1)"
      ],
      "metadata": {
        "id": "ofcRWfG82IbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function\n",
        "df_removed_var=dropping_constant(x)"
      ],
      "metadata": {
        "id": "v43xGnZoWvJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation matrix\n",
        "\n",
        "corr = df_removed_var.corr()\n",
        "cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
        "\n",
        "def magnify():\n",
        "    return [dict(selector=\"th\",\n",
        "                 props=[(\"font-size\", \"7pt\")]),\n",
        "            dict(selector=\"td\",\n",
        "                 props=[('padding', \"0em 0em\")]),\n",
        "            dict(selector=\"th:hover\",\n",
        "                 props=[(\"font-size\", \"12pt\")]),\n",
        "            dict(selector=\"tr:hover td:hover\",\n",
        "                 props=[('max-width', '200px'),\n",
        "                        ('font-size', '12pt')])\n",
        "]\n",
        "\n",
        "corr.style.background_gradient(cmap, axis=1)\\\n",
        "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
        "    .set_caption(\"Hover to magify\")\\\n",
        "    .set_precision(2)\\\n",
        "    .set_table_styles(magnify())"
      ],
      "metadata": {
        "id": "2rkdUtGTWRpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VIF\n",
        "def calc_vif(X):\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "92fcDlV_W2uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Variable Inflation Factor\n",
        "# the independent variables set\n",
        "X = df_removed_var.copy()\n",
        "\n",
        "# VIF dataframe\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "\n",
        "# calculating VIF for each feature\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
        "                          for i in range(len(X.columns))]\n",
        "\n",
        "for i in range(len(vif_data)):\n",
        "  vif_data.loc[i,\"VIF\"]=vif_data.loc[i,\"VIF\"].round(2)\n",
        "  if vif_data.loc[i,\"VIF\"]>=8:\n",
        "    print(vif_data.loc[i,\"feature\"])"
      ],
      "metadata": {
        "id": "SrCB4jSmW50_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Feature Correlation and finding multicolinearity\n",
        "def correlation(df,threshold):\n",
        "  col_corr=set()\n",
        "  corr_matrix= df.corr()\n",
        "  for i in range (len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "      if abs (corr_matrix.iloc[i,j])>threshold:\n",
        "        colname=corr_matrix.columns[i]\n",
        "        col_corr.add(colname)\n",
        "  return list(col_corr)"
      ],
      "metadata": {
        "id": "9RLhJ-oIXlYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation(df_removed_var,0.6)"
      ],
      "metadata": {
        "id": "Y8YRd6DRXn8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping highly correlated values\n",
        "df_removed=df_removed_var.drop(['Email_Source_Type'],axis=1)\n",
        "df_removed.shape"
      ],
      "metadata": {
        "id": "pVkuqUgIYtI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Again checking VIF post-dropped features\n",
        "# the independent variables set\n",
        "X = df_removed.copy()\n",
        "\n",
        "# VIF dataframe\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "\n",
        "\n",
        "# calculating VIF for each feature\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
        "vif_data[\"VIF\"] = vif_data[\"VIF\"].apply(lambda x: round(x, 2))\n",
        "vif_data[vif_data[\"VIF\"] >= 8][\"feature\"].apply(print)"
      ],
      "metadata": {
        "id": "olMTpsRQXt_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df_removed[[i for i in df_removed.describe().columns if i not in ['Email_Status']]])"
      ],
      "metadata": {
        "id": "P0gwR3kLZHb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Dropping Constant Feature, Dropping columns having multicolinearity and validate through VIF.\n",
        "\n",
        "Feature Selector that removes all low variance features. This feature selection algorithm looks only at the features(X), not the desired outputs(Y), and can be used for unsupported learning.\n",
        "\n",
        "A Pearson correlation is a number between -1 and 1 that indicates the extent to which two variables are linearly related. The Pearson correlation is also known as the product moment correlation coefficient (PMCC) or simply correlation\n",
        "\n",
        "Pearson correlations are suitable only for metric variables The correlation coefficient has values between -1 to 1\n",
        "\n",
        " A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n",
        "\n",
        " A value closer to 1 implies stronger positive correlation\n",
        "\n",
        " A value closer to -1 implies stronger negative correlation\n",
        "\n",
        "Collinearity is the state where two variables are highly correlated and contain similar information about the variance within a given dataset. To detect collinearity among variables, simply create a correlation matrix and find variables with large absolute values.\n",
        "\n",
        "Steps for Implementing VIF\n",
        "\n",
        " Calculate the VIF factors.\n",
        "\n",
        " Inspect the factors for each predictor variable, if the VIF is between 510, multicollinearity is likely present and you should consider dropping the variable.\n",
        "\n",
        "In VIF method, we pick each feature and regress it against all of the other features. For each regression, the factor is calculated as :\n",
        "\n",
        "VIF=\\frac{1}{1-R^2}\n",
        "\n",
        "Where, R-squared is the coefficient of determination in linear regression. Its value lies between 0 and 1.\n",
        "\n",
        "1st I dropped columns having constant or quasi constant variance. Then using pearson corelation I removed the columns having multicolinearity and again validate the VIFs for each feauture and found some features having VIF of more than 5-10 and I considered it to be 8 and again manipulated some features and again dropped multicolinear columns to make the VIF less than 8."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#important features\n",
        "df_removed.columns.to_list()"
      ],
      "metadata": {
        "id": "tPJ_JZJuZZCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedded Method of validating the feature importances of selected features\n",
        "def randomforest_embedded(x,y):\n",
        "  # Create the random forest eith hyperparameters\n",
        "  model= RandomForestClassifier(n_estimators=550)\n",
        "  # Fit the mmodel\n",
        "  model.fit(x,y)\n",
        "  # get the importance of thr resulting features\n",
        "  importances= model.feature_importances_\n",
        "  # Create a data frame for visualization\n",
        "  final_df= pd.DataFrame({\"Features\": pd.DataFrame(x).columns, \"Importances\": importances})\n",
        "  final_df.set_index('Importances')\n",
        "  # Sort in ascending order to better visualization\n",
        "  final_df= final_df.sort_values('Importances')\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "MXRAJE9tZcMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting feature importance of selected features\n",
        "randomforest_embedded(x,y=df[\"Email_Status\"])"
      ],
      "metadata": {
        "id": "ezuMx2RbZoDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Transform Your data\n",
        "# Getting symmetric and skew symmetric features from the columns\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in df_removed.describe().columns:\n",
        "  if abs(df_removed[i].mean()-df_removed[i].median())<0.1:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vizualization\n",
        "for variable in non_symmetric_feature:\n",
        "  sns.set_context('notebook')\n",
        "  plt.figure(figsize=(14,5))\n",
        "  plt.subplot(1,2,1)   #means 1 row, 2 Columns and 1st plot\n",
        "  df_removed[variable].hist(bins=30)\n",
        "\n",
        "  ##QQ plot\n",
        "  plt.subplot(1,2,2)\n",
        "  stats.probplot(df_removed[variable], dist='norm',plot=plt)\n",
        "  plt.title(variable)\n",
        "  plt.show()\n",
        "  print('='*120)"
      ],
      "metadata": {
        "id": "yvz2gV3gZ_bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['Subject_Hotness_Score','Total_Past_Communications','Word_Count']:\n",
        "  df_removed[col]=np.sqrt(df_removed[col])"
      ],
      "metadata": {
        "id": "sFkDHrMXaJya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,col in enumerate(['Subject_Hotness_Score','Total_Past_Communications','Word_Count']) :\n",
        "    plt.figure(figsize = (18,18))\n",
        "    plt.subplot(6,2,i+1);\n",
        "    sns.distplot(df_removed[col], color = '#055E85', fit = norm);\n",
        "    feature = df_removed[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n",
        "    plt.title(f'{col.title()}');\n",
        "    plt.tight_layout();"
      ],
      "metadata": {
        "id": "OqwIv9wkaMIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the features, since some of them where having categorial feature therefore did not required transformation, for three features I have applied square root transformation to change it into gaussian normal distribution."
      ],
      "metadata": {
        "id": "Xk6qpMczaTsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x=df1.drop(\"Email_Status\",axis=1)\n",
        "y=df1[\"Email_Status\"]"
      ],
      "metadata": {
        "id": "8p8hC7gQXgZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "#StandardScaler\n",
        "standard_scaler=StandardScaler()\n",
        "standard_scaler_scaled_data=standard_scaler.fit_transform(x)\n",
        "standard_scaler_scaled_data\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset doesnot need any dimensionality reduction.\n",
        "\n",
        "Dimensionality reduction is a technique that is used to reduce the number of features in a dataset. It is often used when the number of features is very large, as this can lead to problems such as overfitting and slow computation. There are a variety of techniques that can be used for dimensionality reduction, such as principal component analysis (PCA) and singular value decomposition (SVD).\n",
        "\n",
        "There are several reasons why dimensionality reduction might be useful. One reason is that it can help to reduce the size of a dataset, which can be particularly useful when the dataset is very large. It can also help to improve the performance of machine learning models by reducing the number of features that the model has to consider, which can lead to faster computation and better generalization to new data.\n",
        "\n",
        "Another reason to use dimensionality reduction is to reduce the curse of dimensionality, which refers to the fact that as the number of dimensions increases, the volume of the space increases exponentially. This can lead to problems such as the nearest neighbor search becoming less effective, as the distances between points become much larger. Dimensionality reduction can help to reduce the curse of dimensionality by reducing the number of dimensions in the data.\n",
        "\n",
        "Finally, dimensionality reduction can also be useful for visualizing high-dimensional data. It can be difficult to visualize data in more than three dimensions, so reducing the number of dimensions can make it easier to understand the patterns in the data."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x_train,x_test,y_train,y_test=train_test_split(standard_scaler_scaled_data,y,test_size=0.2,random_state=45)\n",
        "print(\"The Shape Of X Train Dataset:\",x_train.shape)\n",
        "print(\"The Shape Of X Test Dataset:\",x_test.shape)\n",
        "print(\"The Shape Of y Train Dataset:\",y_train.shape)\n",
        "print(\"The Shape Of y Test Dataset:\",y_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ml model I am spliting data in 80:20 ratio.That means I am using 80% data for trainig the model and 20 % data for testing the model."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependant Variable Column Visualization\n",
        "df['Email_Status'].value_counts().plot(kind='pie',\n",
        "                              figsize=(5,5),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['Ignored','Read','Acknowledged'],\n",
        "                               colors=['lightgreen','yellow','red'],\n",
        "                               explode=[0.1,0.1,0.1]\n",
        "                              );"
      ],
      "metadata": {
        "id": "GLT3vVnzbFJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes.\n",
        "\n",
        "Imbalance means that the number of data points available for different the classes is different: If there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.\n",
        "\n",
        "In our case the dataset dependent column data ratio is 80:16:4. So, during model creating it's obvious that there will be bias and having a great chance of predicting the majority one so frequently. SO the dataset should be balanced before it going for the model creation part."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Email_Status\"].value_counts()"
      ],
      "metadata": {
        "id": "4a6nUrBa9uv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "uVIO7yj9-HNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #SMOTE\n",
        "# # Handaling imbalance dataset using SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "x_train, y_train = sm.fit_resample(x_train, y_train)\n",
        "\n",
        "# describes info about train and test set\n",
        "print(\"Number transactions X_train dataset: \", x_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", x_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counter = Counter(y_train)\n",
        "for key,value in counter.items():\n",
        "  per = value / len(y_train) * 100\n",
        "  print('Class=%d, n=%d (%.3f%%)' % (key, value, per))\n",
        "# plot the distribution\n",
        "plt.bar(counter.keys(), counter.values())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N3HoQ9WvcX5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used SMOTE (Synthetic Minority Over-sampling technique) to balance the 80:16:4 dataset.\n",
        "\n",
        "SMOTE is a technique in machine learning for dealing with issues that arise when working with an unbalanced data set. In practice, unbalanced data sets are common and most ML algorithms are highly prone to unbalanced data so we need to improve their performance by using techniques like SMOTE.\n",
        "\n",
        "To address this disparity, balancing schemes that augment the data to make it more balanced before training the classifier were proposed. Oversampling the minority class by duplicating minority samples or undersampling the majority class is the simplest balancing method.\n",
        "\n",
        "The idea of incorporating synthetic minority samples into tabular data was first proposed in SMOTE, where synthetic minority samples are generated by interpolating pairs of original minority points.\n",
        "\n",
        "SMOTE is a data augmentation algorithm that creates synthetic data points from raw data. SMOTE can be thought of as a more sophisticated version of oversampling or a specific data augmentation algorithm.\n",
        "\n",
        "SMOTE has the advantage of not creating duplicate data points, but rather synthetic data points that differ slightly from the original data points. SMOTE is a superior oversampling option.\n",
        "\n",
        "That's why for lots of advantages, I have used SMOTE technique for balancinmg the dataset."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - LogisticRegression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "model1=LogisticRegression(fit_intercept=True,\n",
        "            class_weight='balanced',multi_class='multinomial')\n",
        "# Fit the Algorithm\n",
        "model1.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "\n",
        "y_train_pred=model1.predict(x_train)\n",
        "y_pred=model1.predict(x_test)\n",
        "\n",
        "training_accuracy=accuracy_score(y_train_pred,y_train)\n",
        "\n",
        "prediction_accuracy=accuracy_score(y_pred,y_test)\n",
        "\n",
        "\n",
        "accuracy1=accuracy_score(y_test,y_pred)\n",
        "precision1=precision_score(y_test,y_pred,average=\"weighted\")\n",
        "recall1=recall_score(y_test,y_pred,average=\"weighted\")\n",
        "f1=f1_score(y_test,y_pred,average=\"weighted\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Accuracy\",training_accuracy * 100,\"%\")\n",
        "print(\"Prediction Accuracy\",prediction_accuracy * 100,\"%\")\n",
        "print(\"Accuracy Of LogisticRegression Model Is: \",accuracy1)\n",
        "print(\"Precision Of LogisticRegression Model Is: \",precision1)\n",
        "print(\"Recall Of LogisticRegression Model Is: \",recall1)\n",
        "print(\"F1 Score Of LogisticRegression Model Is: \",f1)"
      ],
      "metadata": {
        "id": "FsHpBVtFCBwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted probabilities\n",
        "train_pred=model1.predict_proba(x_train)\n",
        "test_pred=model1.predict_proba(x_test)\n"
      ],
      "metadata": {
        "id": "aU2DnvaagQyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_pred=model1.predict(x_train)\n",
        "test_class_pred=model1.predict(x_test)\n"
      ],
      "metadata": {
        "id": "Y_x1AKpJhLe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion Matrix For Training\n",
        "labels=[\"Ignored\",\"Read\",\"Acknowledge\"]\n",
        "cm=confusion_matrix(y_train,train_class_pred)\n",
        "print(cm)\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(cm,annot=True,ax=ax)\n",
        "ax.set_title(\"Confusion matrix For Train\")\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2MCT2sglgPIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion Matrix for Testing\n",
        "labels=[\"Ignored\",\"Read\",\"Acknowledge\"]\n",
        "cm=confusion_matrix(y_test,test_class_pred)\n",
        "print(cm)\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(cm,annot=True,ax=ax)\n",
        "ax.set_title(\"Confusion matrix For Test\")\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3yXr3ZUojtPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Stratified K-Fold Cross-Validation\n",
        "k = 5\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store the scores\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "# Iterate over the folds\n",
        "for train_index, val_index in skf.split(x, y):\n",
        "    # Split the data into training and validation sets\n",
        "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Train the model\n",
        "    model2 = LogisticRegression()\n",
        "    model2.fit(x_train, y_train)\n",
        "\n",
        "    # Perform predictions on the validation set\n",
        "    y_pred = model2.predict(x_val)\n",
        "\n",
        "    # Calculate and append the scores\n",
        "    accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
        "    precision_scores.append(precision_score(y_val, y_pred, average=\"weighted\"))\n",
        "    recall_scores.append(recall_score(y_val, y_pred, average=\"weighted\"))\n",
        "    f1_scores.append(f1_score(y_val, y_pred, average=\"weighted\"))\n",
        "\n",
        "# Print the average scores\n",
        "print(f\"Average Accuracy: {np.mean(accuracy_scores) * 100:.2f}%\")\n",
        "print(f\"Average Precision: {np.mean(precision_scores):.2f}\")\n",
        "print(f\"Average Recall: {np.mean(recall_scores):.2f}\")\n",
        "print(f\"Average F1-Score: {np.mean(f1_scores):.2f}\")"
      ],
      "metadata": {
        "id": "p-aJUj3qtSYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model for train and test data\n",
        "y_pred_train2 = model2.predict(x_train)\n",
        "y_pred2 = model2.predict(x_test)"
      ],
      "metadata": {
        "id": "v7ggbaCVtdSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted probabilities\n",
        "train_probability2 = model2.predict_proba(x_train)\n",
        "test_probability2 = model2.predict_proba(x_test)"
      ],
      "metadata": {
        "id": "KaMQGvLPti3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result for train\n",
        "print(\"Classification Report (Train):\\n\", classification_report(y_train, y_pred_train2))\n",
        "print(\"ROC AUC Score (Train):\", roc_auc_score(y_train, train_probability2, multi_class='ovr'))\n",
        "print()"
      ],
      "metadata": {
        "id": "SweQqFdTtoSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result for test\n",
        "print(\"Classification Report (Test):\\n\", classification_report(y_test, y_pred2))\n",
        "print(\"ROC AUC Score (Test):\", roc_auc_score(y_test, test_probability2, multi_class='ovr'))\n"
      ],
      "metadata": {
        "id": "dNm_0H3vtrF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Stratified KFold Cross validation technique for optimizing performans of the ml model. As in Stratified KFold Cross validation the proportion of different classes remains consistent across the training and validation sets and it gives better results in case of data inbalaced condition."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training dataset, I found precision of 81% and recall of 99% and f1-score of 89% for ignored emails. For class 2 i.e., email opened got precision of 42% and recall of 4% and f1-score of 8% and for class 3 i.e., email acknowledged got precision of 0% and recall of 0% and f1-score of 0% Accuracy is 80% and average precision, recall & f1_score are 41%, 34% and 32% respectively with a roc auc score of 68%.\n",
        "\n",
        "For test dataset, I found precision of 86% and recall of 53% and f1-score of 66% for ignored emails. For class 2 i.e., email opened got precision of 26% and recall of 43% and f1-score of 32% and for class 3 i.e., email acknowledged got precision of 3% and recall of 21% and f1-score of 6% Accuracy is 50% and average precision, recall & f1_score are 38%, 39% and 34% respectively with a roc auc score of 62%."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - RandomForestClassifier"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "model5=RandomForestClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "model5.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "\n",
        "y_train_pred=model5.predict(x_train)\n",
        "y_pred=model5.predict(x_test)\n",
        "\n",
        "training_accuracy5=accuracy_score(y_train_pred,y_train)\n",
        "prediction_accuracy5=accuracy_score(y_test,y_pred)\n",
        "\n",
        "accuracy5=accuracy_score(y_test,y_pred)\n",
        "precision5=precision_score(y_test,y_pred,average=\"weighted\")\n",
        "recall5=recall_score(y_test,y_pred,average=\"weighted\")\n",
        "f5=f1_score(y_test,y_pred,average=\"weighted\")"
      ],
      "metadata": {
        "id": "S7n2m-8lYQFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Accuracy:\",training_accuracy5 * 100)\n",
        "print(\"Prediction Accuracy\",prediction_accuracy5 *100)\n",
        "print(\"Accuracy Of  Model Is: \",accuracy5)\n",
        "print(\"Precision Of  Model Is: \",precision5)\n",
        "print(\"Recall Of  Model Is: \",recall5)\n",
        "print(\"F1 Score Of  Model Is: \",f5)"
      ],
      "metadata": {
        "id": "4QnA13ZiAwSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Predicted Classes\n",
        "train_class_pred22=model5.predict(x_train)\n",
        "test_class_pred22=model5.predict(x_test)"
      ],
      "metadata": {
        "id": "sUnRhfWpoDrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_pred22=model5.predict(x_train)\n",
        "test_class_pred22=model5.predict(x_test)\n"
      ],
      "metadata": {
        "id": "0UEludqdTDz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion Matrix For Training\n",
        "labels=[\"Ignored\",\"Read\",\"Acknowledge\"]\n",
        "cm=confusion_matrix(y_train,train_class_pred22)\n",
        "print(cm)\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(cm,annot=True,ax=ax)\n",
        "ax.set_title(\"Confusion matrix For Train\")\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zgVcSnuyp7AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#confusion Matrix For Testing\n",
        "labels=[\"Ignored\",\"Read\",\"Acknowledge\"]\n",
        "cm=confusion_matrix(y_test,test_class_pred22)\n",
        "print(cm)\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(cm,annot=True,ax=ax)\n",
        "ax.set_title(\"Confusion_matrix For Test\")\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Stratified K-Fold Cross-Validation\n",
        "k = 5\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store the scores\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "# Iterate over the folds\n",
        "for train_index, val_index in skf.split(x, y):\n",
        "    # Split the data into training and validation sets\n",
        "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Train the model\n",
        "    model5 = RandomForestClassifier()\n",
        "    model5.fit(x_train, y_train)\n",
        "\n",
        "    # Perform predictions on the validation set\n",
        "    y_pred = model5.predict(x_val)\n",
        "\n",
        "    # Calculate and append the scores\n",
        "    accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
        "    precision_scores.append(precision_score(y_val, y_pred, average=\"weighted\"))\n",
        "    recall_scores.append(recall_score(y_val, y_pred, average=\"weighted\"))\n",
        "    f1_scores.append(f1_score(y_val, y_pred, average=\"weighted\"))\n",
        "\n",
        "# Print the average scores\n",
        "print(f\"Average Accuracy: {np.mean(accuracy_scores) * 100:.2f}%\")\n",
        "print(f\"Average Precision: {np.mean(precision_scores):.2f}\")\n",
        "print(f\"Average Recall: {np.mean(recall_scores):.2f}\")\n",
        "print(f\"Average F1-Score: {np.mean(f1_scores):.2f}\")"
      ],
      "metadata": {
        "id": "0uGWIlwj2USI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model for train and test data\n",
        "y_pred_train5 = model5.predict(x_train)\n",
        "y_pred5 = model5.predict(x_test)"
      ],
      "metadata": {
        "id": "mwr58u5lRQHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted probabilities\n",
        "train_probability5 = model5.predict_proba(x_train)\n",
        "test_probability5 = model5.predict_proba(x_test)"
      ],
      "metadata": {
        "id": "-Mv58omJRWvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result for train\n",
        "print(\"Classification Report (Train):\\n\", classification_report(y_train, y_pred_train5))\n",
        "print(\"ROC AUC Score (Train):\", roc_auc_score(y_train, train_probability5, multi_class='ovr'))\n",
        "print()"
      ],
      "metadata": {
        "id": "lYJENXMoReJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result for test\n",
        "print(\"Classification Report (Test):\\n\", classification_report(y_test, y_pred5))\n",
        "print(\"ROC AUC Score (Test):\", roc_auc_score(y_test, test_probability5, multi_class='ovr'))\n"
      ],
      "metadata": {
        "id": "eQ-FUQ8SRjxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Stratified KFold Cross validation technique for optimizing performans of the ml model. As in Stratified KFold Cross validation the proportion of different classes remains consistent across the training and validation sets and it gives better results in case of data inbalaced condition."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training dataset, I found precision of 100% and recall of 100% and f1-score of 100% for ignored emails. For class 2 i.e., email opened got precision of 100% and recall of 100% and f1-score of 100% and for class 3 i.e., email acknowledged got precision of 100% and recall of 100% and f1-score of 100% Accuracy is 100% and average precision, recall & f1_score are 100%, 100% and 100% respectively with a roc auc score of 99%.\n",
        "\n",
        "For test dataset, I found precision of 87% and recall of 2% and f1-score of 5% for ignored emails. For class 2 i.e., email opened got precision of 16% and recall of 87% and f1-score of 27% and for class 3 i.e., email acknowledged got precision of 5% and recall of 15% and f1-score of 8% Accuracy is 17% and average precision, recall & f1_score are 36%, 35% and 13% respectively with a roc auc score of 50%."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used these metrices for evaluation of the model and their impact on business are as follows:\n",
        "* **Accuracy**: This metric indicates the percentage of correctly classified instances out of the total number of instances. In a business setting, this would indicate the overall effectiveness of the model in making correct predictions. A high accuracy score would have a positive impact on the business, as it would indicate a high level of confidence in the model's predictions.\n",
        "\n",
        "* **Precision**: This metric indicates the proportion of true positive predictions out of all positive predictions made by the model. In a business setting, this would indicate the level of confidence in the model's ability to identify positive instances correctly. A high precision score would have a positive impact on the business, as it would indicate that the model is not making false positive predictions.\n",
        "\n",
        "* **Recall**: This metric indicates the proportion of true positive predictions out of all actual positive instances. In a business setting, this would indicate the model's ability to identify all positive instances. A high recall score would have a positive impact on the business, as it would indicate that the model is not missing any positive instances.\n",
        "\n",
        "* **F1 Score**: This metric is a combination of precision and recall and is used to balance the trade-off between the two. In a business setting, this would indicate the overall effectiveness of the model in making correct predictions while also avoiding false positives and false negatives. A high F1 score would have a positive impact on the business, as it would indicate that the model is making accurate predictions while also being able to identify all positive instances.\n",
        "\n",
        "* **ROC AUC**: This metric indicates the ability of the model to distinguish between positive and negative instances. In a business setting, this would indicate the model's ability to correctly classify instances as positive or negative. A high ROC AUC score would have a positive impact on the business, as it would indicate that the model is able to correctly classify instances.\n",
        "\n",
        "In summary, the Random Forest Classifier can be considered as an efficient model for the business, especially when it achieves high scores in all of these evaluation metrics, which would indicate that it can accurately predict outcomes, identify all positive instances, and correctly classify instances as positive or negative."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - XGBClassifier"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2=XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "model2.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "\n",
        "y_train_pred=model2.predict(x_train)\n",
        "y_pred=model2.predict(x_test)\n",
        "\n",
        "training_accuracy2=accuracy_score(y_train_pred,y_train)\n",
        "prediction_accuracy2=accuracy_score(y_test,y_pred)\n",
        "\n",
        "accuracy2=accuracy_score(y_test,y_pred)\n",
        "precision2=precision_score(y_test,y_pred,average=\"weighted\")\n",
        "recall2=recall_score(y_test,y_pred,average=\"weighted\")\n",
        "f2=f1_score(y_test,y_pred,average=\"weighted\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Accuracy:\",training_accuracy2 * 100)\n",
        "print(\"Prediction Accuracy\",prediction_accuracy2 *100)\n",
        "print(\"Accuracy Of Model Is: \",accuracy2)\n",
        "print(\"Precision Of Model Is: \",precision2)\n",
        "print(\"Recall Of  Model Is: \",recall2)\n",
        "print(\"F1 Score Of  Model Is: \",f2)"
      ],
      "metadata": {
        "id": "-uTp6EXlSlw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted probabilities\n",
        "train_pred3=model2.predict_proba(x_train)\n",
        "test_pred3=model2.predict_proba(x_test)\n"
      ],
      "metadata": {
        "id": "38JK2GIjTN7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_pred3=model2.predict(x_train)\n",
        "test_class_pred3=model2.predict(x_test)\n"
      ],
      "metadata": {
        "id": "xhcj5L9jTS6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#confusion Matrix For Training\n",
        "labels=[\"Ignored\",\"Read\",\"Acknowledge\"]\n",
        "cm=confusion_matrix(y_train,train_class_pred3)\n",
        "print(cm)\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(cm,annot=True,ax=ax)\n",
        "ax.set_title(\"Confusion matrix For Train\")\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#confusion Matrix For Testing\n",
        "labels=[\"Ignored\",\"Read\",\"Acknowledge\"]\n",
        "cm=confusion_matrix(y_test,test_class_pred3)\n",
        "print(cm)\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(cm,annot=True,ax=ax)\n",
        "ax.set_title(\"Confusion_matrix For Test\")\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1zgBSLxxryWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Stratified K-Fold Cross-Validation\n",
        "k = 5\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store the scores\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "# Iterate over the folds\n",
        "for train_index, val_index in skf.split(x, y):\n",
        "    # Split the data into training and validation sets\n",
        "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Train the model\n",
        "    model2 = XGBClassifier()\n",
        "    model2.fit(x_train, y_train)\n",
        "\n",
        "    # Perform predictions on the validation set\n",
        "    y_pred = model2.predict(x_val)\n",
        "\n",
        "    # Calculate and append the scores\n",
        "    accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
        "    precision_scores.append(precision_score(y_val, y_pred, average=\"weighted\"))\n",
        "    recall_scores.append(recall_score(y_val, y_pred, average=\"weighted\"))\n",
        "    f1_scores.append(f1_score(y_val, y_pred, average=\"weighted\"))\n",
        "\n",
        "# Print the average scores\n",
        "print(f\"Average Accuracy: {np.mean(accuracy_scores) * 100:.2f}%\")\n",
        "print(f\"Average Precision: {np.mean(precision_scores):.2f}\")\n",
        "print(f\"Average Recall: {np.mean(recall_scores):.2f}\")\n",
        "print(f\"Average F1-Score: {np.mean(f1_scores):.2f}\")"
      ],
      "metadata": {
        "id": "KFmkNlwwOqUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model for train and test data\n",
        "y_pred_train2 = model2.predict(x_train)\n",
        "y_pred2 = model2.predict(x_test)"
      ],
      "metadata": {
        "id": "T9gBhhMcStAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted probabilities\n",
        "train_probability2 = model2.predict_proba(x_train)\n",
        "test_probability2 = model2.predict_proba(x_test)"
      ],
      "metadata": {
        "id": "R1kzEBLqSvy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result for train\n",
        "print(\"Classification Report (Train):\\n\", classification_report(y_train, y_pred_train2))\n",
        "print(\"ROC AUC Score (Train):\", roc_auc_score(y_train, train_probability2, multi_class='ovr'))\n",
        "print()"
      ],
      "metadata": {
        "id": "_6O45kBRSynQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result for test\n",
        "print(\"Classification Report (Test):\\n\", classification_report(y_test, y_pred2))\n",
        "print(\"ROC AUC Score (Test):\", roc_auc_score(y_test, test_probability2, multi_class='ovr'))\n"
      ],
      "metadata": {
        "id": "5THXF_r5S2Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Stratified KFold Cross validation technique for optimizing performans of the ml model. As in Stratified KFold Cross validation the proportion of different classes remains consistent across the training and validation sets and it gives better results in case of data inbalaced condition."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training dataset, I found precision of 86% and recall of 98% and f1-score of 91% for ignored emails. For class 2 i.e., email opened got precision of 69% and recall of 32% and f1-score of 43% and for class 3 i.e., email acknowledged got precision of 91% and recall of 9% and f1-score of 17% Accuracy is 84% and average precision, recall & f1_score are 82%, 46% and 50% respectively with a roc auc score of 68%.\n",
        "\n",
        "For test dataset, I found precision of 79% and recall of 1% and f1-score of 1% for ignored emails. For class 2 i.e., email opened got precision of 16% and recall of 99% and f1-score of 28% and for class 3 i.e., email acknowledged got precision of 0% and recall of 0% and f1-score of 0% Accuracy is 17% and average precision, recall & f1_score are 32%, 33% and 10% respectively with a roc auc score of 50%."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating the effectiveness of an email campaign in a classification model, the following evaluation metrics would be considered for a positive business impact:\n",
        "\n",
        "* **Precision**: This metric indicates the proportion of true positive predictions (emails that were opened and resulted in a desired action) out of all positive predictions made by the model. In a business setting, this would indicate the level of confidence in the model's ability to identify individuals who are likely to engage with the campaign. A high precision score would have a positive impact on the business, as it would indicate that the model is not making false positive predictions and is effectively identifying individuals who are likely to engage with the campaign.\n",
        "\n",
        "* **Recall**: This metric indicates the proportion of true positive predictions (emails that were opened and resulted in a desired action) out of all actual positive instances (emails that were opened and resulted in a desired action). In a business setting, this would indicate the model's ability to identify all individuals who engaged with the campaign. A high recall score would have a positive impact on the business, as it would indicate that the model is not missing any individuals who engaged with the campaign.\n",
        "\n",
        "* **F1 Score**: This metric is a combination of precision and recall and is used to balance the trade-off between the two. In a business setting, this would indicate the overall effectiveness of the model in identifying individuals who are likely to engage with the campaign while also avoiding false positives and false negatives. A high F1 score would have a positive impact on the business, as it would indicate that the model is effectively identifying individuals who are likely to engage with the campaign while also being able to identify all individuals who engaged with the campaign.\n",
        "\n",
        "* **ROC AUC**: This metric indicates the ability of the model to distinguish between positive and negative instances. In a business setting, this would indicate the model's ability to correctly classify instances as positive (engaged with the campaign) or negative (did not engage with the campaign). A high ROC AUC score would have a positive impact on the business, as it would indicate that the model is able to correctly classify individuals as likely to engage with the campaign or not.\n",
        "\n",
        "The evaluation metrics that would be considered for a positive business impact of an email campaign effectiveness in a classification model are **precision, recall** which combine to provide F1 score. These metrics would indicate the model's ability to identify individuals who are likely to engage with the campaign while also being able to identify all individuals who engaged with the campaign, and correctly classify instances as positive or negative."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will choose Logistic Regression Model for final prediction, Because of the best performance of the best performance of the model in training as well as prediction of the data. It does not overfit the training dataset compared to the other ml models and the  accuracy,precision,recall and f1-score is also getting increased after using the cross validation techniques.   "
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LogisticRegression :**\n",
        "Training Accuracy 49.43277040092819 % \\\n",
        "Prediction Accuracy 54.45698312454705 % \\\n",
        "Accuracy Of LogisticRegression Model Is:  0.5445698312454705 \\\n",
        "Precision Of LogisticRegression Model Is:  0.757510952520623 \\\n",
        "Recall Of LogisticRegression Model Is:  0.5445698312454705 \\\n",
        "F1 Score Of LogisticRegression Model Is:  0.6210434210218452 \\\n",
        "### **LogisticRegression With Cross validation Techniques :**\n",
        "Average Accuracy: 80.29% \\\n",
        "Average Precision: 0.72 \\\n",
        "Average Recall: 0.80 \\\n",
        "Average F1-Score: 0.73 \\\n",
        "ROC AUC Score (Train): 0.6810871575694524 \\\n",
        "ROC AUC Score (Test): 0.6243740188218597 \\\n",
        "\n",
        "### **RandomForestClassifier:**\n",
        "Training Accuracy: 99.90163849558668 \\\n",
        "Prediction Accuracy 18.31452531317942 \\\n",
        "Accuracy Of  Model Is:  0.1831452531317942 \\\n",
        "Precision Of  Model Is:  0.6766487357671398 \\\n",
        "Recall Of  Model Is:  0.1831452531317942 \\\n",
        "F1 Score Of  Model Is:  0.10333977009590749 \\\n",
        "### **RandomForestClassifier: With Cross validation Techniques :**\n",
        "Average Accuracy: 80.07% \\\n",
        "Average Precision: 0.74 \\\n",
        "Average Recall: 0.80 \\\n",
        "Average F1-Score: 0.76 \\\n",
        "ROC AUC Score (Train): 0.9999968916856035 \\\n",
        "ROC AUC Score (Test): 0.4662545240067582 \\\n",
        "\n",
        "By analysing the above results of two model we can understand that LogisticRegression model has better performance of both training as well as test dataset and having highest ROC curve which is 62% . Thats why I will used LogisticeRegression model for future preference"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ According to the Email Campaign Type feature, Campaign Type 1 had a very high possibility of being viewed even though relatively few emails were sent. The majority of emails sent under email campaign type 2 were ignored. It appears that campaign 3 was successful because more emails were read and acknowledged even though fewer emails were sent during this campaign. \\\n",
        "* Most Of the types of the Emails are the marketting Email having total percentage of 71% and business Email having percentage out of total is 29%. \\\n",
        "* Most Of the Email source types are the marketting Email having total percentage of 54% and Admin Email Of Products having percentage out of total is 46%. \\\n",
        "+ The highest no of the mails send to the customers during evening time.   \\\n",
        "+ G community has maximum population out of all total which is 40.9 % and A community has lowest population which is 2.5 %. \\\n",
        "+ Highest no of the total past communication is done with the G group customer location. \\\n",
        "+ Email_Type 1 has highest no of Emails ignored,read and acknowledge as compared to Email_Type 2.\n",
        "+ The percentage of Ignored are highest which is 80%.\n",
        "The percentage of mails which are read by user are 16% .\n",
        "The percentage of mails which are acknowledge by user are 4 %. \\\n",
        "+ From heatmap we know that there is a negative correlation between Email_Status and Word_count,Total_Links,Total_Images. \\\n",
        "+ There is a positive relation between the total past communications and Email_status. \\\n",
        "+ For final prediction on ml model we are using LogisticRegression ML Model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}